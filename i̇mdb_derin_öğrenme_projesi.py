# -*- coding: utf-8 -*-
"""İMDB_Derin_Öğrenme_Projesi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oMmnl9JlGppwx9IDHmbgZUVP1e5ryRNX

# **İMDB FİLM İNCELEMELERİ İLE DUYGU ANALİZİ PROJESİ**

1.  Muhammet Emin Akyüz-030718106
2.  Veysel Hacı Hazar-030718108
3.  Soner Karaevli-030716005
4.  Yakup Yıldırım-030716034
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive

!ls 'My Drive/COLABS/'

"""**Kullanılacak Kütüphanelerimizi Ekleyelim**

"""

#KÜTÜPHANE BÖLÜMÜ
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from keras.datasets import imdb
from keras.preprocessing.sequence import pad_sequences
from tensorflow.python.keras.preprocessing.sequence import pad_sequences
import numpy as np
import pandas as pd
from tensorflow.python.keras.preprocessing.text import Tokenizer
from tensorflow.python.keras.models  import load_model
from sklearn.model_selection import train_test_split
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, Embedding,LSTM,Dropout
from keras.layers import Dropout
from keras.optimizers import Adam
from keras.layers.recurrent import GRU
import plotly.express as px

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Activation,Dropout
from tensorflow.keras.callbacks import EarlyStopping

"""**Bu Kısımda İMDB Veri Setimizden İlk 15 Verimizin inceleme ve duygu sütunlarını getirdik.**"""

#Veri Yükleme
imdb_verisi = pd.read_csv('My Drive/COLABS/IMDB_Veriseti_v1.csv')
imdb_verisi.head(15)

#Veri Yükleme_2
imdb_verisi_2 = pd.read_csv('My Drive/COLABS/IMDB_Veriseti_v2.csv',low_memory=False)
imdb_verisi_2.head()



"""**Aşağıda yazdığımız imdb_verisi.isna()  metodu ile NaN(Not a number) isimlendirilen verilerimizin ileride problem olmaması için bunları tespit ediyoruz.NaN değerlerinin olduğu satırlarda sonuç True ,olmayan sütunlarda ise False olarak dönüş yapıyor.**"""

imdb_verisi.isna()

imdb_verisi_2.isna()

"""**Print  ile 50000 satırdan ve 2 sütundan oluştuğu öğrenildi.**"""

print(imdb_verisi.shape)

"""**Print  ile 85855 satırdan ve 22 sütundan oluştuğu öğrenildi.**"""

print(imdb_verisi_2.shape)

"""**Duygu sayılarına value_counts() metodu ile bu kısımda bakalım.**"""

imdb_verisi['sentiment'].value_counts()

"""**Ülke sayılarına value_counts() metodu ile bu kısımda baktık.**"""

imdb_verisi_2['country'].value_counts()

"""**Kullanılan dil sayılarına value_counts() metodu ile bu kısımda baktık.**




"""

imdb_verisi_2['language'].value_counts()

"""**Yazarların sayılarına value_counts() metodu ile bu kısımda baktık.**"""

imdb_verisi_2['writer'].value_counts()

"""**imdb_verisi.describe() ile istatistiksel sonuçların hesaplanmasını sağlıyor.**"""

imdb_verisi.describe()

"""**imdb_verisi_2.describe() ile istatistiksel sonuçların hesaplanmasını sağlıyor.**"""

imdb_verisi_2.describe()

"""**Veri setimizin birçok bilgisine bununla ulaşılır.**"""

imdb_verisi.info()

"""**Sütun verilerinin bilgisini verir.**"""

imdb_verisi.columns

"""**Sentiment Sütununu Grafik olarak gösterdik.Verilen positive ve negatif değerler birbirine eşit olduğu görülüyor.**



"""

sns.countplot(imdb_verisi["sentiment"], palette = ["blue","orange"])
plt.show()
print(imdb_verisi.sentiment.value_counts())



"""**IMDB 2. veri setimizin votes ve avg_note kısımlarını matplot kütüphanesi ile şekillendirdik.**"""

fig, vyes = plt.subplots()
vyes.scatter(imdb_verisi_2['avg_vote'], imdb_verisi_2['votes'])
vyes.set_title('IMDB Dataset')
vyes.set_xlabel('avg_vote')
vyes.set_ylabel('votes')

"""**IMDB 2. veri setimizin duration sütun kısmını matplot kütüphanesi ile çektik**

**Sentimentlerimizdeki positiflere 1 değeri ve negatiflere 0 değerini veriyoruz**
"""

imdb_verisi.sentiment = [ 1 if each == "positive" else 0 for each in imdb_verisi.sentiment ]

sentiment = imdb_verisi['sentiment'].values
sentiment

imdb_verisi = imdb_verisi['review']

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(imdb_verisi,sentiment,test_size=0.25,random_state=48)

"""**İngilizce'de en çok kullanılan 18000 kelimeden oluşan bir sözlük oluşturuyoruz.**"""

from tensorflow.python.keras.preprocessing.text import Tokenizer
tokenci=Tokenizer(num_words=18000)
tokenci.fit_on_texts(imdb_verisi)

"""**Farklı uzunluklarda olan yorumlarımız RNN modelini eğitemez.Bu sebepten dolayıda cümleleri eşit boyutta yapmamız gerekiyor.**"""

X_train_Tokens = tokenci.texts_to_sequences(X_train)
X_test_Tokens = tokenci.texts_to_sequences(X_test)

"""**Cümlelerimizdeki her bir kelimenin,yukarıda tanımladığımız ve oluşturduğumuz 18000 kelimeden oluşan sözlüğün herhangi bir indeksle değişiyoruz.**"""

#Verilerimizdeki her cümlemizin kelime sayımını alıp bir liste oluşturuyoruz. 
token_1 = [len(tokens) for tokens in X_train_Tokens + X_test_Tokens]
token_1 = np.array(token_1)

#Token sayıları belirlenirken, ortalama etrafındaki değişiklikler dikkate alınarak sayılar belirlenniyor.
token_2 = np.mean(token_1) + 2 * np.std(token_1)
token_2 = int(token_2)
token_2

"""**token_2= Bu değerimiz verilerimizdeki cümlemizin dağılımını ve eğer varsada zıt uzunluklara sahip cümleleri ortalamaya indirmemize sağlayabileyecektir.**"""

#Belirlediğimiz bu sayının verilerde yüzde kaçına ait olduğu bakılır.
np.sum(token_1 < token_2) / len(token_1)

#Verilimiz belirtilen belirteç sayısına göre belirlenir.
X_train_Padd = pad_sequences(X_train_Tokens, maxlen=token_2)
X_test_Padd = pad_sequences(X_test_Tokens, maxlen=token_2)

X_train_Padd.shape

"""**Görüldüğü gibi şekli 561 e ayarladık**"""

idgrq= tokenci.word_index
ters_h = dict(zip(idgrq.values(), idgrq.keys()))

def ornek_cumle(tokens):
    kelimeler = [ters_h[token] for token in tokens if token!=0]
    yazı = ' '.join(kelimeler)
    return yazı

print(ornek_cumle(X_train_Padd[8]))

print(X_train_Padd[8])

"""**Birinci LSTM modelimiz ile derin öğrenme yaptık.**"""

#LSTM modelimiz ile öğretelim.
model = Sequential()
model.add(Embedding(input_dim=18000,output_dim=60,input_length=token_2,name='embedding_layer1'))

model.add(LSTM(units=14, return_sequences=True))
model.add(Dropout(0.2))

model.add(LSTM(units=7, return_sequences=True))
model.add(Dropout(0.2))

model.add(LSTM(units=3))
model.add(Dropout(0.2))

model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])

model.summary()

history_1 = model.fit(X_train_Padd,Y_train, validation_split=0.28, epochs=6, batch_size=1500, shuffle=True, verbose=1)

result_1 = model.evaluate(X_test_Padd,Y_test)

"""**Result_1'de başarı oranımız yüzde 84 yakaladık.**"""

plt.figure()
plt.plot(history_1.history["accuracy"], label = "Train Kısmı" )
plt.plot(history_1.history["val_accuracy"], label= "Test Kısmı")
plt.title("Model Accuracy")
plt.ylabel("Acc")
plt.xlabel("Epoch")
plt.legend()
plt.show()

plt.figure()
plt.plot(history_1.history["loss"], label= "Train Kısmı ")
plt.plot(history_1.history["val_loss"], label= "Test Kısmı")
plt.title("Model Loss")
plt.ylabel("Loss")
plt.xlabel("Epoch")
plt.legend()
plt.show()

"""**İkinci LSTM modelimizi deniyoruz**"""

model_2=Sequential()

model_2.add(Embedding(input_dim=20000, output_dim=129,input_length=token_2))
model_2.add(LSTM(units=60, activation='tanh'))
model_2.add(Dense(units=1, activation='sigmoid'))

adam= tf.keras.optimizers.Adam(learning_rate=0.0001)

model_2.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])

model_2.summary()

history_2=model_2.fit(X_train_Padd, Y_train, epochs=5,validation_split=0.34, batch_size=128, shuffle=True, verbose=1)

result_2 = model_2.evaluate(X_test_Padd,Y_test)

"""**Result_2 sonucumuzda yüzde 89 başarı oranı yakaladık.**"""

plt.figure()
plt.plot(history_2.history["accuracy"], label = "Train Kısmı" )
plt.plot(history_2.history["val_accuracy"], label= "Test Kısmı")
plt.title("Model Accuracy")
plt.ylabel("Acc")
plt.xlabel("Epoch")
plt.legend()
plt.show()

plt.figure()
plt.plot(history_2.history["loss"], label= "Train Kısmı ")
plt.plot(history_2.history["val_loss"], label= "Test Kısmı")
plt.title("Model Loss")
plt.ylabel("Loss")
plt.xlabel("Epoch")
plt.legend()
plt.show()

"""**GRU katmanı ile deneme yaptık.**"""

model_3=Sequential()

model_3.add(Embedding(input_dim=20000, output_dim=129,input_length=token_2))
model_3.add(GRU(units=60, activation='tanh'))
model_3.add(Dense(units=1, activation='sigmoid'))

adam= tf.keras.optimizers.Adam(learning_rate=0.0001)

model_3.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])

model_3.summary()

history_3=model_3.fit(X_train_Padd, Y_train, epochs=5,validation_split=0.34, batch_size=128, shuffle=True, verbose=1)

result_3 = model_3.evaluate(X_test_Padd,Y_test)

"""**Result_3 sonucumuzda yüzde 89 başarı oranı yakaladık.**"""

plt.figure()
plt.plot(history_3.history["accuracy"], label = "Train Kısmı" )
plt.plot(history_3.history["val_accuracy"], label= "Test Kısmı")
plt.title("Model Accuracy")
plt.ylabel("Acc")
plt.xlabel("Epoch")
plt.legend()
plt.show()

plt.figure()
plt.plot(history_3.history["loss"], label= "Train Kısmı ")
plt.plot(history_3.history["val_loss"], label= "Test Kısmı")
plt.title("Model Loss")
plt.ylabel("Loss")
plt.xlabel("Epoch")
plt.legend()
plt.show()

"""**İkinci veri seti**"""

imdb_verisi_2 = pd.read_csv('My Drive/COLABS/IMDB_Veriseti_v2.csv',low_memory=False)
imdb_verisi_2.head(3)

"""**Genre sütunu ile ikinci veri seti görselleştirdik.**"""

Visualization = px.pie(values=imdb_verisi_2['genre'].value_counts(), 
             names=imdb_verisi_2['genre'].value_counts().index,title='Genre sütununun içeriği')

Visualization.show()

imdb_verisi_2.nunique()#Her satırda benzersiz içerik bulmak için kullanılır.

imdb_verisi_2.isnull().sum()#NaN mevcut olan her sütunun sayısını verecektir.

imdb_verisi_2.country.fillna("Country Unvailable",inplace=True)

imdb_verisi_2.isnull().sum()



"""**Üçüncü veri seti**"""

veri_seti_3 = pd.read_csv('My Drive/COLABS/veri_seti_3.csv')
print(veri_seti_3.shape)
veri_seti_3.head()

veri_seti_3.nunique()#Her satırda benzersiz içerik bulmak için kullanılır.

veri_seti_3.isnull().sum()#NaN mevcut olan her sütunun sayısını verecektir.

"""**3.veri setimizdeki verileri temizleyeceğiz**

**Yukarıdaki sonuçlardan; director,cast,country,date_added ve rating sütunlarının eksik değerlere sahip olduğunu görebiliriz.İlk olarak,bu eksik değerleri ele alıyorum.**


"""

veri_seti_3.director.fillna("No Director",inplace=True)
veri_seti_3.cast.fillna("No Cast",inplace=True)
veri_seti_3.country.fillna("Country Unvailable",inplace=True)
veri_seti_3.dropna(subset=["date_added","rating"],inplace=True)

veri_seti_3.isnull().sum()

"""**İkinci veri setimizdeki ilk 5 satırımızdaki title ve ratings sütunlarını listeledik**"""

yeni_ratings = pd.DataFrame({'Title':imdb_verisi_2.title,
                             'Rating':imdb_verisi_2.avg_vote})
yeni_ratings.drop_duplicates(subset=['Title','Rating'],inplace=True)

print(yeni_ratings.shape)
yeni_ratings.head(5)

ınner_verisi = yeni_ratings.merge(veri_seti_3,left_on='Title',right_on='title',how='inner')
ınner_verisi = ınner_verisi.sort_values(by='Rating',ascending=False)

print(ınner_verisi.shape)
ınner_verisi.head(5)

Yeni_Data=ınner_verisi[['Title','Rating','type']]

Yeni_Data.drop_duplicates(subset=['Title','Rating','type'],inplace=True)
print(Yeni_Data.shape)
Yeni_Data.head(5)

Filmler_Data = Yeni_Data[Yeni_Data.type == 'Movie']
TVS_Data =  Yeni_Data[Yeni_Data.type == 'TV Show']
print(Filmler_Data.shape)
print(TVS_Data.shape)

Filmler_Data = Filmler_Data.drop(['type'], axis = 1)

Filmler_Data

Filmler_Data['Polarity_Rating'] = Filmler_Data['Rating'].apply(lambda x:'Positive' if x > 6 else 'Negative')
Filmler_Data

sekil = px.pie(values=Filmler_Data['Polarity_Rating'].value_counts(),
               names=Filmler_Data['Polarity_Rating'].value_counts().index)
sekil.show()

pozitivs = Filmler_Data[Filmler_Data['Polarity_Rating'] == 'Positive']
negativs = Filmler_Data[Filmler_Data['Polarity_Rating'] == 'Negative']

print(pozitivs.shape)
print(negativs.shape)

ea = Filmler_Data[['Title','Polarity_Rating']]
ea

one_kods = pd.get_dummies(ea["Polarity_Rating"])
ea.drop(['Polarity_Rating'],axis=1,inplace=True)
ea = pd.concat([ea,one_kods],axis=1)
ea

X= ea['Title'].values
y= ea.drop('Title',axis=1).values
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.10,random_state=42)

X_train

y_train

vctr = CountVectorizer()
X_train = vctr.fit_transform(X_train)
X_test = vctr.transform(X_test)

sss = TfidfTransformer()
X_train =sss.fit_transform(X_train)
X_test = sss.transform(X_test)
X_train = X_train.toarray()
X_test = X_test.toarray()

model_4 = Sequential()

model_4.add(Dense(units=12673,activation ='relu'))
model_4.add(Dropout(0.2))

model_4.add(Dense(units=4000,activation ='relu'))
model_4.add(Dropout(0.2))

model_4.add(Dense(units=500,activation ='relu'))
model_4.add(Dropout(0.2))

model_4.add(Dense(units=2,activation ='sigmoid'))

opwt=tf.keras.optimizers.Adam(learning_rate=0.001)
model_4.compile(loss='binary_crossentropy',optimizer=opwt,metrics=['binary_accuracy'])

erken_durs = EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=4)

wxy=model_4.fit(x=X_train, y=y_train, batch_size=50, epochs=80,validation_data=(X_test,y_test),verbose=1,callbacks=erken_durs)
wxy

result_4 = model_4.evaluate(X_test,y_test,batch_size=64,verbose=1)
print('Test Accuracy Başarı Oranı:',result_4[1])

cıktı = pd.DataFrame(wxy.history)

cıktı.loc[1:, ['loss','val_loss']].plot()
cıktı.loc[1:, ['binary_accuracy','val_binary_accuracy']].plot()

print(("En iyi Validation Loss: {:0.4f}"+\
       "\n En iyi Validation Accuracy: {:0.4f}")\
       .format(cıktı['val_loss'].min(),
               cıktı['val_binary_accuracy'].max()))





